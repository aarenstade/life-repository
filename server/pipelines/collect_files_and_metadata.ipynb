{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "from services.files.directory_file_organizer import DirectoryFileOrganizer\n",
    "from services.files.metadata_extraction import FileMetadataExtractor\n",
    "\n",
    "directory_path = \"../../data/files\"\n",
    "\n",
    "directory_file_organizer = DirectoryFileOrganizer()\n",
    "all_files = directory_file_organizer.list_files_recursive(directory_path)\n",
    "all_files = directory_file_organizer.group_files_by_type(all_files)\n",
    "flattened_files = [file for sublist in all_files.values() for file in sublist]\n",
    "len(flattened_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "all_files_w_metadata = FileMetadataExtractor().extract_files_metadata_concurrent(\n",
    "    flattened_files, max_workers=24\n",
    ")\n",
    "\n",
    "with open(\"all_files_w_metadata.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_files_w_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"all_files_w_metadata.pkl\", \"rb\") as f:\n",
    "    loaded_files_w_metadata = pickle.load(f)\n",
    "\n",
    "loaded_files_w_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "from models.files import File\n",
    "from models.file_metadata import (\n",
    "    ImageMetadata,\n",
    "    VideoMetadata,\n",
    "    TextMetadata,\n",
    "    AudioMetadata,\n",
    "    ArchiveMetadata,\n",
    "    DocumentMetadata,\n",
    ")\n",
    "\n",
    "from utilities.general import generate_id\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "\n",
    "def convert_file_to_serializable_dicts(\n",
    "    file: File,\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    def format_datetime(dt):\n",
    "        return dt.strftime(\"%Y-%m-%d %H:%M:%S\") if dt else None\n",
    "\n",
    "    file_dict = {\n",
    "        \"id\": file.id,\n",
    "        \"name\": file.name,\n",
    "        \"type\": file.type.value,\n",
    "        \"path\": file.path,\n",
    "    }\n",
    "\n",
    "    metadata_dict = {\n",
    "        \"id\": generate_id(),\n",
    "        \"file_id\": file.id,\n",
    "        \"size\": file.metadata.size,\n",
    "        \"created_at\": format_datetime(file.metadata.created_at),\n",
    "        \"modified_at\": format_datetime(file.metadata.modified_at),\n",
    "    }\n",
    "\n",
    "    if isinstance(file.metadata, ImageMetadata):\n",
    "        metadata_dict.update(\n",
    "            {\n",
    "                \"image_width\": file.metadata.width,\n",
    "                \"image_height\": file.metadata.height,\n",
    "                \"image_color_mode\": file.metadata.color_mode,\n",
    "                \"image_format\": file.metadata.format,\n",
    "                \"image_location\": (\n",
    "                    {\n",
    "                        \"latitude\": file.metadata.location[0],\n",
    "                        \"longitude\": file.metadata.location[1],\n",
    "                    }\n",
    "                    if file.metadata.location\n",
    "                    else None\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    elif isinstance(file.metadata, VideoMetadata):\n",
    "        metadata_dict.update(\n",
    "            {\n",
    "                \"video_duration\": file.metadata.duration,\n",
    "                \"video_width\": file.metadata.width,\n",
    "                \"video_height\": file.metadata.height,\n",
    "                \"video_framerate\": file.metadata.framerate,\n",
    "                \"video_codec\": file.metadata.codec,\n",
    "                \"video_bitrate\": file.metadata.bitrate,\n",
    "                \"video_location\": (\n",
    "                    {\n",
    "                        \"latitude\": file.metadata.location[0],\n",
    "                        \"longitude\": file.metadata.location[1],\n",
    "                    }\n",
    "                    if file.metadata.location\n",
    "                    else None\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "    elif isinstance(file.metadata, TextMetadata):\n",
    "        metadata_dict.update(\n",
    "            {\n",
    "                \"text_num_words\": file.metadata.num_words,\n",
    "                \"text_language\": file.metadata.language,\n",
    "                \"text_encoding\": file.metadata.encoding,\n",
    "            }\n",
    "        )\n",
    "    elif isinstance(file.metadata, AudioMetadata):\n",
    "        metadata_dict.update(\n",
    "            {\n",
    "                \"audio_bitrate\": file.metadata.bitrate,\n",
    "                \"audio_duration\": file.metadata.duration,\n",
    "                \"audio_sample_rate\": file.metadata.sample_rate,\n",
    "                \"audio_channels\": file.metadata.channels,\n",
    "                \"audio_codec\": file.metadata.codec,\n",
    "            }\n",
    "        )\n",
    "    elif isinstance(file.metadata, ArchiveMetadata):\n",
    "        metadata_dict.update(\n",
    "            {\n",
    "                \"archive_num_files\": file.metadata.num_files,\n",
    "                \"archive_compression_type\": file.metadata.compression_type,\n",
    "                \"archive_encrypted\": file.metadata.encrypted,\n",
    "            }\n",
    "        )\n",
    "    elif isinstance(file.metadata, DocumentMetadata):\n",
    "        metadata_dict.update(\n",
    "            {\n",
    "                \"document_num_pages\": file.metadata.num_pages,\n",
    "                \"document_author\": file.metadata.author,\n",
    "                \"document_title\": file.metadata.title,\n",
    "                \"document_language\": file.metadata.language,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return file_dict, metadata_dict\n",
    "\n",
    "\n",
    "def convert_files_concurrently(files):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(convert_file_to_serializable_dicts, files))\n",
    "    return results\n",
    "\n",
    "\n",
    "file_dicts = convert_files_concurrently(loaded_files_w_metadata)\n",
    "file_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dbio.supabase import SupabaseDatabaseAdapter\n",
    "\n",
    "db = SupabaseDatabaseAdapter(\n",
    "    url=os.environ[\"SUPABASE_URL\"], key=os.environ[\"SUPABASE_SECRET_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "files = pd.DataFrame([file[0] for file in file_dicts])\n",
    "metadata = pd.DataFrame([file[1] for file in file_dicts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata.applymap(\n",
    "    lambda x: int(x) if isinstance(x, str) and x.isdigit() else x\n",
    ")\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_dicts:\n",
    "    db.insert(\"files\", file[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "metadata.replace({pd.NA: None}, inplace=True)\n",
    "metadata.replace({np.nan: None}, inplace=True)\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def clean_and_insert_metadata(metadata_df):\n",
    "    def clean_row(row):\n",
    "        row_dict = row.to_dict()\n",
    "        for key, value in row_dict.items():\n",
    "            if isinstance(value, str):\n",
    "                if value.isdigit():\n",
    "                    row_dict[key] = int(value)\n",
    "        cleaned_dict = json.loads(\n",
    "            json.dumps(row_dict, default=str).replace(\"NaN\", \"null\")\n",
    "        )\n",
    "        return cleaned_dict\n",
    "\n",
    "    for _, row in metadata_df.iterrows():\n",
    "        cleaned_row = clean_row(row)\n",
    "        db.insert(\"file_metadata\", cleaned_row)\n",
    "\n",
    "\n",
    "clean_and_insert_metadata(metadata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
